{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHhe2SQ3ZcaZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQoGnKKxzLaQ"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "\n",
        "# Database options\n",
        "!pip install chromadb # if you use chromadb as your vector database\n",
        "\n",
        "# Others\n",
        "!pip install langchain-community # if you use langchain for orchastration\n",
        "!pip install transformers #if you use huggingface for vector embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-C4OfFwypO3"
      },
      "outputs": [],
      "source": [
        "# enable GPU if needed, GPU can speed up your vector embedding if you computing these vectors locally (not using API)\n",
        "\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqTOQq1J0XuN"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import chromadb\n",
        "import openai\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Set OpenAI API Key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylTv6cpKy3mo",
        "outputId": "7d1852c8-6e63-4dc0-b72e-923933194a98"
      },
      "outputs": [],
      "source": [
        "# Load the Drive and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3P_C0YC0qzI"
      },
      "outputs": [],
      "source": [
        "folder_path = \"/content/drive/Shared drives/Datathon/Data/hackathon_data/\"# Google drive path of the dataset\n",
        "files_in_folder = os.listdir(folder_path)\n",
        "\n",
        "len(files_in_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LjRErs62K45"
      },
      "outputs": [],
      "source": [
        "print(\"total file number:\", len(files_in_folder))\n",
        "for f in files_in_folder[:10]:\n",
        "  p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggyAhRXJ0tar"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# file paths\n",
        "json_path = \"/content/drive/Shared drives/Datathon/Data/hackathon_data/1-act.com.json\"\n",
        "\n",
        "# JSON loading\n",
        "with open(json_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# extract desired text(contents inside 'text_by_page_url')\n",
        "page_texts = data.get(\"text_by_page_url\", {})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV-NCJvk3Ri2"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", grouped_entities=True)\n",
        "entities = ner(text[:1000])  # we usually limit for performance\n",
        "\n",
        "for e in entities:\n",
        "    print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2P6lRYj3aF5"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# open the json\n",
        "json_path = \"/content/drive/Shared drives/Datathon/Data/hackathon_data/1-act.com.json\"\n",
        "with open(json_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# bring real data from 'text_by_page_url'\n",
        "text = data[\"text_by_page_url\"][\"http://1-act.com/\"]\n",
        "\n",
        "# calculating TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform([text])\n",
        "\n",
        "# extracting Top keywords\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "tfidf_scores = X.toarray()[0]\n",
        "top_indices = np.argsort(tfidf_scores)[::-1][:20]\n",
        "\n",
        "print(\"\\nðŸ”‘ Top Keywords:\")\n",
        "for i in top_indices:\n",
        "    print(f\"{feature_names[i]}: {tfidf_scores[i]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXgxTyFk2jX6"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 1. break text entries (by line breaks)\n",
        "items = [line.strip() for line in text.split(\"\\n\") if len(line.strip()) > 3]\n",
        "\n",
        "# 2. TF-IDF vecterization\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(items)\n",
        "\n",
        "# 3. clustering (ex: divide to 5 clusters)\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# 4. print the results\n",
        "for i, label in enumerate(labels):\n",
        "    print(f\"[Cluster {label}] {items[i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kCOgd_U5cZj"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hH1ikTQGBCBf"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# download model in local and execute\n",
        "pipe = pipeline(\"text-generation\", model=\"google/flan-t5-base\", max_new_tokens=128)\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHZu9QGpMbap"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
        "chunks = text_splitter.split_text(text)\n",
        "documents = [Document(page_content=chunk) for chunk in chunks]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRHT6UoGMcH0"
      },
      "outputs": [],
      "source": [
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})  # only return 2 most relevant chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EoUczqDMb5H"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfQ9EuFAMGaa"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "\n",
        "persist_directory = tempfile.mkdtemp()  # creates a temp folder for chroma to work in\n",
        "\n",
        "\n",
        "# split into short chunks\n",
        "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
        "chunks = text_splitter.split_text(text)\n",
        "documents = [Document(page_content=chunk) for chunk in chunks]\n",
        "\n",
        "# embed and build vector DB\n",
        "embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "vector_db = Chroma.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "# HuggingFace model\n",
        "from transformers import pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_new_tokens=256)\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Build retriever with limited context\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# Final RAG chain\n",
        "from langchain.chains import RetrievalQA\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
        "\n",
        "# Ask question\n",
        "query = \"Where is it located?\"\n",
        "response = qa.run(query)\n",
        "print(response)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
