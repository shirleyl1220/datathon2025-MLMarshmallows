{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-5_QawHMw60q","outputId":"3a013af5-03b2-4410-ae9b-5fe242fab128","executionInfo":{"status":"ok","timestamp":1743892440188,"user_tz":-120,"elapsed":24777,"user":{"displayName":"Pia Rosebelle dela Paz","userId":"08459596554345452689"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.70.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n","Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.0)\n","Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n","Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.1)\n","Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.7.6)\n","Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\n","Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n","Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.26.4)\n","Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.23.0)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.1)\n","Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.21.0)\n","Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n","Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.52b1)\n","Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.31.1)\n","Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n","Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n","Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n","Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n","Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n","Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n","Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n","Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n","Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.16)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n","Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n","Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n","Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n","Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n","Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n","Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n","Requirement already satisfied: opentelemetry-proto==1.31.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n","Requirement already satisfied: opentelemetry-instrumentation-asgi==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n","Requirement already satisfied: opentelemetry-instrumentation==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n","Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n","Requirement already satisfied: opentelemetry-util-http==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n","Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n","Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n","Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n","Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.1)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n","Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n","Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n","Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n","Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n","Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n","Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.21)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.51)\n","Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.23)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.8.1)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.22)\n","Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n","Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.3.1)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (0.3.8)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (2.11.1)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (4.13.1)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-community) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (2.33.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.4.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.0)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n","Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","Collecting en-core-web-sm==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n"]}],"source":["# !pip install \"protobuf<6.0.0,>=3.20.3\" --force-reinstall\n","# !pip install numpy==1.26.4 --force-reinstall\n","!pip install openai\n","!pip install chromadb\n","!pip install langchain-community\n","!pip install transformers\n","!pip install --upgrade torch transformers\n","!pip install spacy\n","!python -m spacy download en_core_web_sm\n","!pip install sentence-transformers"]},{"cell_type":"code","source":["import re\n","import os\n","import json\n","import numpy\n","import time\n","import tempfile\n","import spacy\n","import openai\n","from typing import List\n","\n","import chromadb\n","from chromadb.utils import embedding_functions\n","from chromadb.config import Settings\n","\n","from sklearn.cluster import KMeans\n","from transformers import pipeline\n","from transformers import GPT2Tokenizer\n","from sentence_transformers import SentenceTransformer\n","\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.embeddings import OpenAIEmbeddings\n","from langchain.schema import Document\n","from langchain.vectorstores import Chroma\n","from langchain.llms import HuggingFacePipeline\n","from langchain.llms import OpenAI\n","from langchain.chat_models import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","from langchain.chains import LLMChain\n","from langchain.chains import RetrievalQA\n","\n","import torch\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")"],"metadata":{"id":"1g_Lal1Tyjz_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743892451964,"user_tz":-120,"elapsed":11771,"user":{"displayName":"Pia Rosebelle dela Paz","userId":"08459596554345452689"}},"outputId":"a2fcc79c-b3b0-4cef-8c57-5f93600d976a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["os.environ[\"OPENAI_API_KEY\"] = \"\""],"metadata":{"id":"gGl0R2F-Fb9h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the Drive and mount\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","folder_path = \"/content/drive/Shared drives/Datathon/Data/hackathon_data/\"# Google drive path of the dataset\n","json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]"],"metadata":{"id":"-d_R6SGXw7zC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743892452957,"user_tz":-120,"elapsed":989,"user":{"displayName":"Pia Rosebelle dela Paz","userId":"08459596554345452689"}},"outputId":"25d1b0f6-1925-4e40-dfb3-cb3e62d0c97a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["folder_path = \"/content/drive/Shared drives/Datathon/Data/hackathon_data/\"# Google drive path of the dataset\n","json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]"],"metadata":{"id":"ziZDPlaPw-YD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DB setup\n","persist_directory = tempfile.mkdtemp()\n","chroma_client = chromadb.Client(Settings(persist_directory=persist_directory, anonymized_telemetry=False))\n","collection = chroma_client.get_or_create_collection(name=\"biz_web_chunks\")\n","\n","# Embedder model\n","embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","# LLM\n","pipe = pipeline(\"text2text-generation\", model=\"t5-small\")\n","llm = HuggingFacePipeline(pipeline=pipe)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"id1vyMV7kCMq","executionInfo":{"status":"ok","timestamp":1743892456358,"user_tz":-120,"elapsed":3175,"user":{"displayName":"Pia Rosebelle dela Paz","userId":"08459596554345452689"}},"outputId":"d3dc0d32-791f-4f63-907b-13002139095b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Device set to use cuda:0\n","<ipython-input-6-073fc2db3311>:11: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n","  llm = HuggingFacePipeline(pipeline=pipe)\n"]}]},{"cell_type":"code","source":["tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","nlp = spacy.load(\"en_core_web_sm\")\n","max_token = 128\n","MAX_CHUNK_SIZE = 1000000\n","\n","def load_text_from_json(path):\n","    # with open(path, \"r\") as f:\n","    #     data = json.load(f)\n","\n","    # # Handle multiple URLs - concatenate their content into one large text block\n","    # all_text = \"\"\n","    # for url, page_text in data.get(\"text_by_page_url\", {}).items():\n","    #     all_text += f\"\\n\\n--- URL: {url} ---\\n\\n{page_text}\"\n","\n","    # return data[\"url\"], all_text\n","\n","    with open(path, \"r\") as f:\n","        data = json.load(f)\n","\n","    if \"text_by_page_url\" not in data:\n","        print(f\"Warning: No 'text_by_page_url' found in {path}\")\n","        return data.get(\"url\", \"unknown\"), \"\"\n","\n","    all_text = \"\"\n","    for url, page_text in data[\"text_by_page_url\"].items():\n","        all_text += f\"\\n\\n--- URL: {url} ---\\n\\n{page_text}\"\n","\n","    return data.get(\"url\", \"unknown\"), all_text\n","\n","def embed_sentences(text):\n","    \"\"\"Embeds sentences using a pre-trained Sentence-BERT model.\"\"\"\n","    # Check the text length before passing to spaCy\n","    if len(text) > nlp.max_length:\n","        print(f\"Warning: Text exceeds {nlp.max_length} characters, splitting into smaller chunks.\")\n","        # Split the text into smaller chunks manually\n","        text_chunks = chunk_text_by_token_limit(text, max_tokens=128)\n","        all_sentences = []\n","        all_embeddings = []\n","\n","        for chunk in text_chunks:\n","            # Process each chunk with spaCy\n","            doc = nlp(chunk)\n","            sentences = [sent.text.strip() for sent in doc.sents]\n","            embeddings = embedding_model.encode(sentences, convert_to_tensor=True)\n","\n","            all_sentences.extend(sentences)\n","            all_embeddings.extend(embeddings)\n","\n","        return all_sentences, all_embeddings\n","\n","    else:\n","        # If the text is under the limit, process it normally\n","        doc = nlp(text)\n","        sentences = [sent.text.strip() for sent in doc.sents]\n","        embeddings = embedding_model.encode(sentences, convert_to_tensor=True)\n","        return sentences, embeddings\n","\n","def extract_entities(text):\n","    \"\"\"Extracts named entities from text using spaCy's NER.\"\"\"\n","    doc = nlp(text)\n","    entities = [ent.label_ for ent in doc.ents]  # Extract entity labels\n","    return list(set(entities))  # Remove duplicates and return as a list\n","\n","\"\"\"\n","DIFFERENT CHUNKING TECHNIQUES\n","\n","To use during our experimentation, we considered chunking data in various ways.\n","\n","def paragraph_chunk(text, max_chars=1000): chunks text by considering paragraphs\n","def chunk_text_by_token_limit(text, max_tokens=128): chunks text by considering a set limit of tokens\n","semantic_chunking(raw_text, max_tokens=128): chunks text by considering semantic similarity\n","\"\"\"\n","\n","def split_into_sentences(text: str) -> List[str]:\n","    \"\"\"Splits the text into sentences using spaCy's sentence boundary detection.\"\"\"\n","    doc = nlp(text)\n","    return [sent.text.strip() for sent in doc.sents]\n","\n","def paragraph_chunk(text, max_chars=1000):\n","    paragraphs = [str(p).strip() for p in text.split(\"\\n\") if str(p).strip()]\n","    chunks = []\n","    current = \"\"\n","    for p in paragraphs:\n","        if len(current) + len(p) < max_chars:\n","            current += \" \" + p\n","        else:\n","            chunks.append(current.strip())\n","            current = p\n","    if current:\n","        chunks.append(current.strip())\n","    return chunks\n","\n","def chunk_text_by_length_limit(text, max_chars=MAX_CHUNK_SIZE):\n","    \"\"\"Chunks text into pieces of max_chars length, trying to respect sentence boundaries.\"\"\"\n","    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n","\n","    chunks = []\n","    current_chunk = \"\"\n","\n","    for sentence in sentences:\n","        if len(current_chunk) + len(sentence) + 1 <= max_chars:\n","            current_chunk += \" \" + sentence if current_chunk else sentence\n","        else:\n","            if current_chunk:\n","                chunks.append(current_chunk.strip())\n","            current_chunk = sentence\n","\n","    if current_chunk:\n","        chunks.append(current_chunk.strip())\n","\n","    return chunks\n","\n","def chunk_text_by_token_limit(text, max_tokens=128):\n","    # Tokenize the text and calculate token count\n","    tokens = tokenizer.encode(text, truncation=False)\n","    token_count = len(tokens)\n","\n","    # If the text is too long, chunk it into parts that fit within the token limit\n","    chunks = []\n","    current_chunk = []\n","    current_token_count = 0\n","\n","    for token in tokens:\n","        current_token_count += 1\n","        current_chunk.append(token)\n","\n","        # If adding the next token exceeds max_tokens, save the current chunk and start a new one\n","        if current_token_count > max_tokens:\n","            chunks.append(tokenizer.decode(current_chunk, skip_special_tokens=True))\n","            current_chunk = [token]  # Start a new chunk with the current token\n","            current_token_count = 1  # Reset token count for the new chunk\n","\n","    # Add the last chunk if it exists\n","    if current_chunk:\n","        chunks.append(tokenizer.decode(current_chunk, skip_special_tokens=True))\n","\n","    return chunks\n","\n","def semantic_chunking(text, max_chunk_size=1_000_000, max_tokens=128):\n","    \"\"\"Performs semantic chunking with fallback to token-based splitting for large chunks.\"\"\"\n","    # Step 1: Split into sentences\n","    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n","    if not sentences:\n","        return []\n","\n","    # Step 2: Embed sentences\n","    embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n","    sentence_embeddings = embedding.embed_documents(sentences)\n","\n","    # Step 3: Cluster sentences\n","    num_clusters = max(2, len(sentences) // 5)\n","    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n","    labels = kmeans.fit_predict(sentence_embeddings)\n","\n","    # Step 4: Group and conditionally split large chunks\n","    clustered_texts = {}\n","    for label, sentence in zip(labels, sentences):\n","        clustered_texts.setdefault(label, []).append(sentence)\n","\n","    chunks = []\n","    for label in sorted(clustered_texts.keys()):\n","        chunk = \" \".join(clustered_texts[label])\n","        if len(chunk) > max_chunk_size:\n","            print(f\"[!] Large chunk ({len(chunk)} chars), using length-based fallback.\")\n","            sub_chunks = chunk_text_by_length_limit(chunk, max_chars=max_chunk_size)\n","            chunks.extend(sub_chunks)\n","        else:\n","            chunks.append(chunk)\n","\n","    return chunks\n","\n","def create_documents(text_chunks, source_url):\n","    \"\"\"Creates LangChain Document objects from the text chunks for a specific URL.\"\"\"\n","    docs = []\n","    for chunk in text_chunks:\n","        if not chunk.strip():\n","            continue\n","\n","        # Extract entities for the chunk\n","        entities = extract_entities(chunk)\n","        labels_string = \",\".join(entities)  # Convert the list of entities to a string\n","\n","        # Create a Document object with metadata\n","        doc = Document(\n","            page_content=chunk,\n","            metadata={\n","                \"source\": source_url,  # Store the source URL\n","                \"labels\": labels_string  # Store the extracted NER labels as metadata\n","            }\n","        )\n","        docs.append(doc)\n","\n","    return docs\n","\n","\"\"\"\n","DIFFERENT .JSON PROCESSING FUNCTIONS\n","\n","To use during our experimentation, we have written multiple .json processing functions.\n","\n","process_specific_json_files(json_folder_path, specific_files): processes specific files through the pipeline to check the results\n","process_all_json_files(json_folder_path): processes all files through the pipeline to check the results\n","\"\"\"\n","def process_specific_json_files(json_folder_path, specific_files):\n","    # Store all documents here\n","    all_documents = []\n","\n","    # Process only the specific files provided in the list\n","    for i, json_file in enumerate(specific_files):\n","        file_path = os.path.join(json_folder_path, json_file)\n","\n","        if os.path.exists(file_path):  # Ensure the file exists\n","            print(f\"Processing {json_file} ({i+1}/{len(specific_files)})...\")  # Showing progress\n","\n","            # Load the content from the JSON file\n","            source_url, all_text = load_text_from_json(file_path)\n","\n","            # Split the text into chunks based on your chunking logic (e.g., paragraph chunking, semantic chunking, etc.)\n","            text_chunks = semantic_chunking(all_text)\n","\n","            # Create documents for each chunk\n","            docs = create_documents(text_chunks, source_url)  # Now we pass both the chunks and the source URL\n","            all_documents.extend(docs)  # Add the resulting documents to the list\n","\n","    return all_documents\n","\n","def process_all_json_files(json_folder_path):\n","    # List all JSON files in the directory\n","    json_files = [f for f in os.listdir(json_folder_path) if f.endswith('.json')]\n","\n","    # Store all documents here\n","    all_documents = []\n","\n","    files_processed = 0\n","\n","    # Process all files\n","    for json_file in json_files:\n","        file_path = os.path.join(json_folder_path, json_file)\n","        print(f\"Processing {json_file}...\")\n","\n","        # Create documents for each JSON file\n","        docs = create_documents(file_path)  # Call the create_documents function from before\n","        all_documents.extend(docs)  # Add the resulting documents to the list\n","\n","        files_processed += 1\n","    return all_documents\n","\n","def retrieve_relevant_documents(query, vectorstore, top_k=5):\n","    \"\"\"Retrieve the most relevant documents based on a query.\"\"\"\n","    # Step 1: Embed the query\n","    query_embedding = embedding_model.encode([query], convert_to_tensor=True)\n","\n","    # Step 2: Perform semantic search using Chroma's similarity search\n","    search_results = vectorstore.similarity_search_by_vector(query_embedding[0], k=top_k)\n","\n","    # Step 3: Return the top-k relevant documents\n","    return search_results\n","\n","# Function to generate an answer from the LLM using retrieved documents\n","def generate_answer_from_documents(query, relevant_docs):\n","    \"\"\"Generate an answer using the LLM and the context provided by the retrieved documents.\"\"\"\n","    # Combine the retrieved documents' content into a single text block\n","    document_content = \"\\n\".join([doc.page_content for doc in relevant_docs])\n","\n","    # Create a prompt to ask the LLM for an answer based on the context\n","    prompt = f\"\"\"\n","    You are an AI assistant that answers questions based on the context below. For each answer, please provide your reasoning or explain how you arrived at the conclusion.\n","\n","    Context:\n","    {document_content}\n","\n","    Question: {query}\n","\n","    Answer: Based on the context, please explain your reasoning for the answer, and then provide the final answer. If you can't find enough information to answer the question, say \"I don't know.\"\n","    \"\"\"\n","\n","    # Use the LLM to generate an answer\n","    response = llm(prompt)\n","\n","    return response.strip()"],"metadata":{"id":"iN2MkTt3w_pp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def create_documents(json_path):\n","#     # Load and combine all URLs' content into one block\n","#     source, raw_text = load_text_from_json(json_path)\n","\n","#     # Deduplicate BEFORE chunking\n","#     # deduped_text = deduplicate_text(raw_text)\n","\n","#     # Chunk the deduplicated text\n","#     # chunks = paragraph_chunk(raw_text)\n","#     chunks = semantic_chunking(raw_text)\n","\n","#     # Just for debugging\n","#     print(f\"Total Chunks from {json_path}: {len(chunks)}\")\n","\n","\n","#     # Create LangChain Document objects\n","#     docs = []\n","#     for chunk in chunks:\n","#         if not chunk or not isinstance(chunk, str) or not chunk.strip():\n","#             continue\n","#         labels = extract_entities(chunk)\n","#         labels_string = \",\".join(labels)\n","#         doc = Document(\n","#             page_content=chunk,\n","#             metadata={\n","#                 \"source\": source,\n","#                 \"labels\": labels_string\n","#             }\n","#         )\n","#         docs.append(doc)\n","#     return docs\n","\n","# def extract_entities_from_query(query):\n","#     doc = nlp(query)\n","#     entities = [ent.label_ for ent in doc.ents]\n","#     return entities\n","\n","# def embed_sentences(text):\n","#     \"\"\"Embeds sentences using a pre-trained Sentence-BERT model.\"\"\"\n","#     # Split the text into sentences using spaCy\n","#     doc = nlp(text)\n","#     sentences = [sent.text.strip() for sent in doc.sents]\n","\n","#     # Embed each sentence\n","#     embeddings = embedding_model.encode(sentences, convert_to_tensor=True)\n","#     return sentences, embeddings\n","\n","# # Function to filter documents based on query entities\n","# def filter_documents_by_entities(documents, query_entities):\n","#     relevant_documents = []\n","\n","#     for doc in documents:\n","#         metadata = doc.metadata.get(\"labels\", \"\").split(\",\")  # Assuming metadata contains labels as a comma-separated string\n","#         if any(entity in metadata for entity in query_entities):\n","#             relevant_documents.append(doc)\n","\n","#     return relevant_documents\n","\n","# # Function to generate an answer based on relevant documents\n","# def generate_answer_from_relevant_docs(query, relevant_docs):\n","#     # Access the page_content attribute of each Document object\n","#     documents_text = \"\\n\".join([doc.page_content for doc in relevant_docs])  # Fixed this line\n","\n","#     prompt = f\"\"\"\n","#     You are a helpful assistant. Please provide a detailed, formal response to the questions you are asked.\n","#     Below are some documents related to the question. Your answers must be based on thes documents.\n","#     Please read them and provide the best possible answer to the question. If you don't know the answer, just say that you don't know, don't try to make up an answer.:\n","#     Documents:\n","#     {documents_text}\n","\n","#     Question: {query}\n","#     Answer:\n","#     \"\"\"\n","\n","#     response = llm(prompt, max_length=150, num_return_sequences=1)\n","#     return response"],"metadata":{"id":"AZMrrTFEBOpX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start_time = time.time()\n","\n","specific_files = ['ippathways.com.json',\n","                  'auroraarizona.com.json',\n","                  'covenantwoods.com.json',\n","                  'amsfulfillment.com.json',\n","                  'starmark.com.json',\n","                  'cariloha.com.json',\n","                  '12stone.com.json',\n","                  'beautifuldestinations.com.json',\n","                  'act-on.com.json',\n","                  '1-act.com.json']\n","all_documents = process_specific_json_files(folder_path, specific_files)\n","\n","embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n","vectorstore = Chroma.from_documents(\n","    documents=all_documents,\n","    embedding=embedding_model,\n","    persist_directory=persist_directory\n","    )\n","\n","end_time = time.time()\n","processing_time = end_time - start_time\n","print(f\"All the .json files processed in {processing_time:.2f} seconds.\")"],"metadata":{"id":"foyJ1sQhiomt","executionInfo":{"status":"ok","timestamp":1743896365290,"user_tz":-120,"elapsed":276075,"user":{"displayName":"Pia Rosebelle dela Paz","userId":"08459596554345452689"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5bb2f853-e6fd-4a9c-a7fe-ff8087edd115"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing ippathways.com.json (1/10)...\n","[!] Large chunk (1158610 chars), using length-based fallback.\n","Processing auroraarizona.com.json (2/10)...\n","Processing covenantwoods.com.json (3/10)...\n","Processing amsfulfillment.com.json (4/10)...\n","Processing starmark.com.json (5/10)...\n","Processing cariloha.com.json (6/10)...\n","Processing 12stone.com.json (7/10)...\n","Processing beautifuldestinations.com.json (8/10)...\n","Processing act-on.com.json (9/10)...\n","Processing 1-act.com.json (10/10)...\n","All the .json files processed in 276.01 seconds.\n"]}]},{"cell_type":"code","source":["query = \"Who uses Agile Methodologies to deal with Marketing in Fort Lauderdale, FL?\"\n","top_k = 3  # Number of relevant documents to retrieve\n","\n","# Retrieve relevant documents from Chroma\n","query_embedding = embedding_model.embed_query(query)\n","\n","# Retrieve documents from the vector store\n","relevant_docs = vectorstore.similarity_search_by_vector(query_embedding, k=top_k)\n","\n","# Generate an answer using the LLM\n","answer = generate_answer_from_documents(query, relevant_docs)\n","\n","print(f\"Answer to the query '{query}':\")\n","print(answer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6I93RZ_FYYd","executionInfo":{"status":"ok","timestamp":1743896848831,"user_tz":-120,"elapsed":332,"user":{"displayName":"Pia Rosebelle dela Paz","userId":"08459596554345452689"}},"outputId":"adb194be-7f1d-4b11-f8e3-112615591034"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Answer to the query 'Who uses Agile Methodologies to deal with Marketing in Fort Lauderdale, FL?':\n","LAB Agency Services Subscription Culture People Careers Work Trending Articles eTips\n"]}]},{"cell_type":"code","source":["# # vectorstore = Chroma(collection_name=\"biz_web_chunks\", embedding_function=embedding_model, client=chroma_client)\n","# retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n","\n","# pipe = pipeline(\"text2text-generation\", model=\"t5-small\")\n","# llm = HuggingFacePipeline(pipeline=pipe)\n","\n","# qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n","\n","# # Ask question\n","# query = \"What company provides assisted living near Richmond, Virginia?\"\n","# retrieved_docs = retriever.get_relevant_documents(query)\n","\n","# # Extract entities from the query and filter documents by these entities\n","# entities_in_query = extract_entities_from_query(query)\n","# relevant_docs = filter_documents_by_entities(retrieved_docs, entities_in_query)\n","\n","# # Generate the answer from relevant documents\n","# answer = generate_answer_from_relevant_docs(query, relevant_docs)\n","# print(f\"Answer: {answer}\")\n","\n","# # answer = generate_answer_from_relevant_docs(query, relevant_docs)\n","# # print(f\"Answer: {answer}\")"],"metadata":{"id":"aSHcFeJLxY7S"},"execution_count":null,"outputs":[]}]}